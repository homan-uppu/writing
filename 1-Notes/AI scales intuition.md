
> "The intuitive mind is a sacred gift and the rational mind is a faithful servant" - Einstein.

AI enables us to share our intuition with others, not just the products of our intuition.

By intuition, I mean, how someone observes, understands, values, predicts.












---

The power of AI is in its ability to represent our intuition. Intuition is the breakthrough that deep learning brought. Before we trained models that represented intuition, computers could only calculate.

Calculation only gets you so far, and falls short for most complex problems which are intractable: you’d never be able to calculate your way to figuring out what product to build, or what movie to make in the same way you could calculate the solution for a board game.

Deep learning with the transformer architecture and sufficient scale enabled computers to have intuition. In the same we have a “feel” for something instinctively without conscious, rational thought, AI has a feel for things that it has picked up through training.

But what we’ve realized is that current state of the art AI systems cannot replicate our intuition well. Our intuitions are quite complex.

People look at LLMs and criticize them for not being able to reason or calculate as if that were the holy grail. Intuition is actually the more difficult challenge. Reasoning is much easier than intuition.

Intuition determines which options out of the many that you choose at each step. You cannot simply reason your way to profound solutions because there are far too many options to choose from at each step.

If you don’t see many options then your intuition has prevented you from seeing it: the options have been pruned subconsciously. That too is the work of intuition

Elites in any domain have made this clear. Einstein, Terrence Tao, Steve Jobs and ofc artists have all talked about how important intuition is in order to do their work well.

We are incredibly early on. The degree of intuitions that current models can capture is stupidly limited.

When people say we’re going to have AGI soon because of some “takeoff”, I think they’re mistaken about what it takes to get to AGI because they underestimate the computational complexity of human intuition.

I’m not saying we won’t get there. All the incentives point to AGI: it is the pinnacle of our species, it is what we are programmed to try and create. But it will take much longer than it may seem to some.

When people talk about “self improving” what does that mean exactly? Updating its code for how it trains itself? Well, the training algorithm isn’t the primary bottleneck for AGI, it’s the computational scale required to represent the complexity of human intuition in a model. 

We will need far more compute, and even fundamental step function improvements in the underlying compute architecture (e.g quantum computers) in order to get anywhere reasonably close to human level intuition.

And even if we had the compute, we lack rich training data. Let’s consider what data are used to train AI systems: language, images, audio, video. By using a ton of data across these mediums a model is able to form associations between concepts: words to images, sketches to words, etc. and so far it feels magical to see it.

But imagine what it takes to, say, build a product people want, or create art that people want. Can that intuition even be encoded in words, images, videos, etc? It can’t.

Predicting the next token is the right model because that is how we fundamentally operate (reaction to stimuli), but we lack the richness in the data required to build a great model.

We will need to be able to represent human feeling and use that as part of the training data. What do I mean by feeling? The sensations we feel when we engage with things in the world.

Ex: “Liking” something on social media is an extremely reductive representation of what we feel about a particular post. Two people looking at the same thing likely feel different things conditioned by their past experiences and individual nature. Even when we both feel anger, there’s a difference between our individual angers.

And yet our feelings determine our actions. We buy products that make us feel a certain way. We appreciate art that makes us feel. AI that can match us in creating things people want will need to understand this human “feeling”.

Yet, we don’t even capture feeling in the data we train AI on. How can an AI system build intuition for how to design a great product, or make art if it’s blind to feeling?

Even with infinite compute, you would not be able to build an AI system that can compete with the best of us unless it is also trained on human feeling: what we feel in response to stimuli: words, images, smells, visuals, audio, etc. - because that is what our models are trained on, not just words.

Since the incentives exist, I’m sure we will eventually create products that will capture such data. And we will willingly use them since they will offer an experience we cannot live without. And indirectly this richer data can be used to improve AI.

This will take time. We don’t have such products yet (but maybe they are around the corner that I’m not aware of).

AI is on the path to codify our intuition. It will take much longer than it may seem, but all the incentives point to us trying to do so.

—-

- Needs a better title, etc. I think I have the gist of my feeling down. Need to also talk about the difficulty in translating intuition into words and why intuition is where the magic lies (use the Steve Jobs example earlier).
- Models trained on others work isn’t an argument that will last because humans do the same thing. Our intuitions and what we choose to create is almost entirely based on what we’ve consumed. As we see AI more like humans, this backlash against training on human data will perish. I.e we won’t care about this eventually as we realize there are no “truly original” creations. Everything we create is built upon the work of others we’ve consumed. We can’t take true “ownership” of anything in the way some artists claim. (More philosophical argument perhaps).